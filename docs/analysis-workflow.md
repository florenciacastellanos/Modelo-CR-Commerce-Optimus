# üîÑ Workflow de An√°lisis de Contact Rate

## Visi√≥n General

Este documento describe el flujo completo para realizar un an√°lisis de Contact Rate, desde la configuraci√≥n inicial hasta la generaci√≥n de insights accionables.

## Flujo Completo (6 Pasos)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASO 1: CONFIGURACI√ìN                                      ‚îÇ
‚îÇ  ‚Ä¢ Definir periodo (fechas)                                 ‚îÇ
‚îÇ  ‚Ä¢ Seleccionar sites (pa√≠ses)                               ‚îÇ
‚îÇ  ‚Ä¢ Seleccionar Commerce Groups                              ‚îÇ
‚îÇ  ‚Ä¢ Aplicar filtros opcionales (User Types, Environments)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASO 2: EXTRACCI√ìN DE DATOS                                ‚îÇ
‚îÇ  ‚Ä¢ Ejecutar query BigQuery                                  ‚îÇ
‚îÇ  ‚Ä¢ Aplicar exclusiones autom√°ticas                          ‚îÇ
‚îÇ  ‚Ä¢ Calcular AGRUP_COMMERCE                                  ‚îÇ
‚îÇ  ‚Ä¢ Agregar por dimensi√≥n seleccionada                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASO 3: CONFIGURACI√ìN DE DRIVERS                           ‚îÇ
‚îÇ  ‚Ä¢ Por cada Site                                            ‚îÇ
‚îÇ  ‚Ä¢ Por cada Periodo detectado                               ‚îÇ
‚îÇ  ‚Ä¢ Configurar valor de Driver manualmente                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASO 4: C√ÅLCULO DE CONTACT RATE                            ‚îÇ
‚îÇ  ‚Ä¢ CR = (Incoming Cases / Driver) √ó 100                     ‚îÇ
‚îÇ  ‚Ä¢ Calcular para cada combinaci√≥n Site √ó Periodo            ‚îÇ
‚îÇ  ‚Ä¢ Validar resultados                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASO 5: AN√ÅLISIS DE VARIACIONES                            ‚îÇ
‚îÇ  ‚Ä¢ Variaci√≥n absoluta (pp)                                  ‚îÇ
‚îÇ  ‚Ä¢ Variaci√≥n relativa (%)                                   ‚îÇ
‚îÇ  ‚Ä¢ Impacto en volumen                                       ‚îÇ
‚îÇ  ‚Ä¢ Detectar patrones (spikes, drops, strong variations)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PASO 6: INTERPRETACI√ìN Y ACCIONABLES                       ‚îÇ
‚îÇ  ‚Ä¢ Identificar top drivers de variaci√≥n                     ‚îÇ
‚îÇ  ‚Ä¢ Analizar distribuci√≥n temporal                           ‚îÇ
‚îÇ  ‚Ä¢ Comparar con periodos anteriores                         ‚îÇ
‚îÇ  ‚Ä¢ Generar insights y recomendaciones                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## PASO 1: Configuraci√≥n

### 1.1 Definir Periodo

**Objetivo:** Establecer el rango de fechas para el an√°lisis.

**Opciones:**
- **Monthly:** An√°lisis mes a mes
- **Quarterly:** An√°lisis trimestral

**Ejemplo:**
```python
analysis_parameters = {
    'start_date': '2026-01-01',
    'end_date': '2026-02-28',
    'date_format': 'month'
}
```

**Resultado:** Sistema detecta autom√°ticamente periodos completos:
- 2026-01 (Enero 2026)
- 2026-02 (Febrero 2026)

### 1.2 Seleccionar Sites

**Objetivo:** Definir pa√≠ses a analizar.

**Opciones disponibles:**
- MLA (Argentina)
- MLB (Brasil) ‚ö†Ô∏è Requiere sampling
- MLC (Chile)
- MCO (Colombia)
- MLM (M√©xico)
- MLU (Uruguay)
- MPE (Per√∫)

**Ejemplo:**
```python
analysis_parameters['selected_sites'] = ['MLA', 'MLC', 'MCO']
```

**Consideraci√≥n:** MLB genera alto volumen ‚Üí sampling autom√°tico.

### 1.3 Seleccionar Commerce Groups

**Objetivo:** Definir grupos de Commerce a analizar.

**Opciones disponibles:** 15 Commerce Groups en 5 categor√≠as.

**Ejemplo:**
```python
analysis_parameters['selected_agrup_commerce'] = ['PDD', 'PNR', 'ME Distribuci√≥n']
```

**Tip:** Comenzar con 2-3 grupos para an√°lisis inicial.

### 1.4 Filtros Opcionales

#### User Types
```python
analysis_parameters['selected_user_types'] = ['COMPRADOR', 'VENDEDOR']
# Si se omite o se incluyen los 3, no se aplica filtro
```

#### Environments
```python
analysis_parameters['selected_environments'] = ['DS', 'FBM']
# Si se omite o est√° vac√≠o, no se aplica filtro
```

### 1.5 Dimensi√≥n de An√°lisis

**Objetivo:** Definir c√≥mo se agrupar√°n los datos.

**Opciones:**
- PROCESS (Process Name)
- CDU (Caso de Uso)
- REASON_DETAIL (Reason Detail Group)
- COMMERCE_GROUP (Commerce Group)
- REPORTING_TYPE (Reporting Type)
- ENVIRONMENT (Environment)
- VERTICAL (‚ö†Ô∏è NULL actualmente)
- DOMAIN (‚ö†Ô∏è NULL actualmente)

**Ejemplo:**
```python
analysis_parameters['output_dimension'] = 'PROCESS'
```

### 1.6 Threshold de Casos

**Objetivo:** Filtrar dimensiones con volumen insuficiente manteniendo procesos significativos.

**‚ö†Ô∏è REGLA VALIDADA (Enero 2026):** 

**Si la SUMA TOTAL de un PROCESS_NAME es >= 50 casos en CUALQUIER per√≠odo de comparaci√≥n,
se incluyen TODOS los CDUs/dimensiones de ese proceso.**

```python
MIN_PROCESS_INCOMING = 50  # M√≠nimo de casos TOTALES por proceso por per√≠odo
```

**L√≥gica de aplicaci√≥n:**
```python
# 1. Calcular total por proceso en cada per√≠odo
process_totals = df.groupby('PROCESS_NAME').agg({
    'INCOMING_PERIOD1': 'sum',
    'INCOMING_PERIOD2': 'sum'
}).reset_index()

# 2. Identificar procesos que cumplen el threshold
valid_processes = process_totals[
    (process_totals['INCOMING_PERIOD1'] >= 50) | 
    (process_totals['INCOMING_PERIOD2'] >= 50)
]['PROCESS_NAME'].tolist()

# 3. Incluir TODOS los CDUs de procesos v√°lidos
df = df[df['PROCESS_NAME'].isin(valid_processes)]
```

**Beneficios validados:**
- ‚úÖ Captura procesos significativos con CDUs distribuidos
- ‚úÖ No pierde informaci√≥n relevante
- ‚úÖ Permite an√°lisis completo (ej: "Post Compra Posterior a la Entrega ME" con 146 casos en 16 CDUs)

**Threshold agregado (opcional):**
```python
analysis_parameters['custom_threshold'] = 100  # Para nivel agregado (default)
```

---

## PASO 2: Extracci√≥n de Datos

### 2.1 Construir Query BigQuery

**Archivo:** `/sql/base-query.sql`

**Proceso:**
1. Reemplazar placeholders con par√°metros
2. Aplicar filtros de sites, commerce groups, user types, environments
3. Ejecutar query en BigQuery

**Ejemplo de ejecuci√≥n:**
```python
from melitk.bigquery import BigQueryClientBuilder

connector = BigQueryClientBuilder().with_encoded_secret('DME000131_DEV').build()

final_query = BASE_QUERY.format(
    fecha_inicio='2026-01-01',
    fecha_fin='2026-02-28',
    sites="'MLA', 'MLC'",
    agrup_commerce="'PDD', 'PNR'",
    user_types="'Comprador', 'Vendedor'",
    environment_filter=""  # o filtro espec√≠fico
)

response = connector.query_to_df(final_query)
data = response.df
```

### 2.2 Validar Datos Extra√≠dos

**Checks:**
```python
# 1. Volumen total
print(f"Total cases: {data['CANT_CASES'].sum()}")

# 2. Distribuci√≥n por site
print(data.groupby('SIT_SITE_ID')['CANT_CASES'].sum())

# 3. Distribuci√≥n por Commerce Group
print(data.groupby('AGRUP_COMMERCE')['CANT_CASES'].sum())

# 4. Periodos detectados
print(data['MES'].unique())
```

### 2.3 Optimizar Memoria (si aplica)

**Trigger:** Datasets > 50,000 filas

**Proceso autom√°tico:**
```python
from utils.memory_optimization import optimize_dataframe_memory

data = optimize_dataframe_memory(data)
```

**Resultado:** 50-70% reducci√≥n de memoria.

### 2.4 Aplicar Threshold

**Filtrar dimensiones con volumen insuficiente:**
```python
threshold = analysis_parameters['custom_threshold']

# Agregar por dimensi√≥n
agg_data = data.groupby(['SIT_SITE_ID', 'MES', 'PROCESS_NAME']).agg({
    'CANT_CASES': 'sum',
    'CAS_CASE_ID': 'nunique'
}).reset_index()

# Aplicar threshold
agg_data = agg_data[agg_data['CANT_CASES'] >= threshold]
```

---

## PASO 3: Configuraci√≥n de Drivers

### 3.1 Detectar Periodos y Sites

**Autom√°tico:** El sistema detecta combinaciones √∫nicas de Site √ó Periodo.

**Ejemplo:**
```python
sites = ['MLA', 'MLC']
periodos = ['2026-01', '2026-02']

# Resultado: 4 combinaciones
# MLA √ó 2026-01
# MLA √ó 2026-02
# MLC √ó 2026-01
# MLC √ó 2026-02
```

### 3.2 Configurar Drivers Manualmente

**Proceso:**

Para cada combinaci√≥n Site √ó Periodo, configurar valor de Driver:

```python
drivers_by_site = {
    'MLA': {
        '2026-01': 1500000,  # 1.5M √≥rdenes en Enero MLA
        '2026-02': 1600000   # 1.6M √≥rdenes en Febrero MLA
    },
    'MLC': {
        '2026-01': 250000,   # 250K √≥rdenes en Enero MLC
        '2026-02': 270000    # 270K √≥rdenes en Febrero MLC
    }
}
```

**Fuentes de Drivers:**
- Dashboards internos
- Tablas de √≥rdenes (BT_ORD_ORDERS)
- Reportes de BI
- Datos hist√≥ricos

### 3.3 Validar Drivers

**Checks:**
```python
# 1. Todos los periodos tienen driver
for site in sites:
    for periodo in periodos:
        assert drivers_by_site[site][periodo] > 0, f"Missing driver for {site} √ó {periodo}"

# 2. Drivers son coherentes (variaci√≥n < 50% MoM)
for site in sites:
    values = list(drivers_by_site[site].values())
    for i in range(1, len(values)):
        variation_pct = abs((values[i] - values[i-1]) / values[i-1] * 100)
        if variation_pct > 50:
            print(f"‚ö†Ô∏è Warning: {site} driver variation > 50% MoM")
```

---

## PASO 4: C√°lculo de Contact Rate

### 4.1 F√≥rmula

```python
def calculate_contact_rate(incoming, driver):
    """
    Calculate Contact Rate in percentage points (pp)
    
    Args:
        incoming (float): Total incoming cases
        driver (float): Driver value
    
    Returns:
        float: Contact Rate in pp (4 decimals)
    """
    if driver and driver > 0:
        return round((incoming / driver) * 100, 4)
    return None
```

### 4.2 Aplicar C√°lculo

```python
# Agregar incoming por Site √ó Periodo
incoming_by_site_periodo = data.groupby(['SIT_SITE_ID', 'MES'])['CANT_CASES'].sum().reset_index()
incoming_by_site_periodo.rename(columns={'CANT_CASES': 'INCOMING_CASES'}, inplace=True)

# Agregar drivers
incoming_by_site_periodo['DRIVER'] = incoming_by_site_periodo.apply(
    lambda row: drivers_by_site.get(row['SIT_SITE_ID'], {}).get(row['MES'], 0),
    axis=1
)

# Calcular CR
incoming_by_site_periodo['CR'] = incoming_by_site_periodo.apply(
    lambda row: calculate_contact_rate(row['INCOMING_CASES'], row['DRIVER']),
    axis=1
)
```

### 4.3 Validar Resultados

```python
# 1. CRs dentro de rango esperado (0.5 - 15.0 pp)
assert incoming_by_site_periodo['CR'].min() >= 0.5, "CR too low - check data"
assert incoming_by_site_periodo['CR'].max() <= 15.0, "CR too high - check data"

# 2. No hay NULLs
assert incoming_by_site_periodo['CR'].notna().all(), "NULL CRs found"

# 3. Visualizar
print(incoming_by_site_periodo)
```

---

## PASO 5: An√°lisis de Variaciones

### 5.1 Variaci√≥n Absoluta (pp)

```python
# Ordenar por Site y Periodo
incoming_by_site_periodo = incoming_by_site_periodo.sort_values(['SIT_SITE_ID', 'MES'])

# Calcular variaci√≥n absoluta
incoming_by_site_periodo['CR_PREV'] = incoming_by_site_periodo.groupby('SIT_SITE_ID')['CR'].shift(1)
incoming_by_site_periodo['VAR_ABS_PP'] = incoming_by_site_periodo['CR'] - incoming_by_site_periodo['CR_PREV']
```

### 5.2 Variaci√≥n Relativa (%)

```python
incoming_by_site_periodo['VAR_REL_PCT'] = (
    (incoming_by_site_periodo['CR'] - incoming_by_site_periodo['CR_PREV']) / 
    incoming_by_site_periodo['CR_PREV']
) * 100
```

### 5.3 Impacto en Volumen

```python
incoming_by_site_periodo['VOLUME_IMPACT'] = (
    incoming_by_site_periodo['VAR_ABS_PP'] / 100 * incoming_by_site_periodo['DRIVER']
)
```

### 5.4 Detectar Patrones

```python
from calculations.pattern_detection import detect_patterns

patterns = detect_patterns(incoming_by_site_periodo)

# Resultado:
# - spikes: [list of detected spikes]
# - drops: [list of detected drops]
# - strong_variations: [list of strong MoM variations]
# - concentrations: [list of temporal concentrations]
```

---

## PASO 6: Interpretaci√≥n y Accionables

### 6.1 Identificar Top Drivers de Variaci√≥n

**Por dimensi√≥n seleccionada:**

```python
# Agregar por dimensi√≥n (ej: PROCESS_NAME)
variations_by_process = data.groupby(['SIT_SITE_ID', 'MES', 'PROCESS_NAME']).agg({
    'CANT_CASES': 'sum'
}).reset_index()

# Calcular CR por proceso
# ... (similar a paso 4)

# Ordenar por variaci√≥n absoluta
top_variations = variations_by_process.sort_values('VAR_ABS_PP', ascending=False).head(10)

print("Top 10 Drivers de Variaci√≥n:")
print(top_variations[['PROCESS_NAME', 'VAR_ABS_PP', 'VAR_REL_PCT', 'VOLUME_IMPACT']])
```

### 6.2 Analizar Distribuci√≥n Temporal

**Por d√≠a del mes:**

```python
# Extraer d√≠a del mes
data['DAY'] = data['CONTACT_DATE_ID'].dt.day

# Distribuci√≥n diaria
daily_dist = data.groupby(['SIT_SITE_ID', 'MES', 'DAY'])['CANT_CASES'].sum().reset_index()

# Detectar d√≠as de concentraci√≥n
threshold_concentration = 0.30
for site in sites:
    for periodo in periodos:
        subset = daily_dist[(daily_dist['SIT_SITE_ID'] == site) & (daily_dist['MES'] == periodo)]
        total_cases = subset['CANT_CASES'].sum()
        
        # Top 3 d√≠as
        top3_days = subset.nlargest(3, 'CANT_CASES')
        top3_cases = top3_days['CANT_CASES'].sum()
        concentration = top3_cases / total_cases
        
        if concentration > threshold_concentration:
            print(f"‚ö†Ô∏è {site} √ó {periodo}: Concentration = {concentration:.2%}")
            print(f"   Top 3 days: {top3_days['DAY'].tolist()}")
```

### 6.3 Comparar con Periodos Anteriores

**Tendencia hist√≥rica:**

```python
# Si hay datos de meses anteriores
historical_data = ...  # cargar datos hist√≥ricos

# Calcular CR hist√≥rico
historical_cr = calculate_historical_cr(historical_data, drivers_historical)

# Comparar tendencia
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(historical_cr['MES'], historical_cr['CR'], marker='o', label='CR Hist√≥rico')
plt.axhline(y=historical_cr['CR'].mean(), color='r', linestyle='--', label='Promedio')
plt.xlabel('Periodo')
plt.ylabel('Contact Rate (pp)')
plt.title('Tendencia de CR - MLA')
plt.legend()
plt.grid(True)
plt.show()
```

### 6.4 Generar Insights y Recomendaciones

**Template de reporte:**

```markdown
## Executive Summary - Contact Rate Analysis

### Periodo Analizado
- Start: {start_date}
- End: {end_date}
- Sites: {sites}
- Commerce Groups: {commerce_groups}

### Hallazgos Principales

1. **Variaci√≥n Total:**
   - MLA: {var_mla_pp} pp ({var_mla_pct}%)
   - MLC: {var_mlc_pp} pp ({var_mlc_pct}%)

2. **Top Drivers de Variaci√≥n:**
   - {process_1}: +{var_1} pp (impacto: +{impact_1} casos)
   - {process_2}: +{var_2} pp (impacto: +{impact_2} casos)
   - {process_3}: -{var_3} pp (impacto: -{impact_3} casos)

3. **Patrones Detectados:**
   - {num_spikes} Spikes detectados
   - {num_drops} Drops detectados
   - {num_strong_vars} Strong Variations

### Recomendaciones

1. **Inmediatas:**
   - Investigar causa ra√≠z de spike en {process_spike}
   - Validar mejora en {process_improvement}

2. **Corto Plazo:**
   - Implementar mejoras en {process_target}
   - Monitorear evoluci√≥n de {commerce_group}

3. **Mediano Plazo:**
   - Automatizar procesos con alto CR
   - Mejorar documentaci√≥n/FAQs
```

---

## Consideraciones Especiales

### MLB (Brasil) - Sampling

**Problema:** Volumen extremadamente alto causa timeouts.

**Soluci√≥n:** Aplicar sampling sistem√°tico.

**Referencia:** `/sql/sampling-strategy.sql`

**Proceso:**
1. Estimar filas totales: `num_agrup √ó num_months √ó 50,000`
2. Si estimaci√≥n > 150,000 ‚Üí aplicar sampling
3. L√≠mite min: 150,000 filas
4. L√≠mite max: 200,000 filas
5. M√©todo: `ORDER BY RAND() LIMIT {limit}`

### VERTICAL y DOMAIN NULL

**Problema:** Campos NULL por tabla no disponible.

**Workaround:** Usar otras 6 dimensiones disponibles.

**Esperado:** Resoluci√≥n futura cuando tabla se identifique.

### Memory Optimization

**Trigger:** Datasets > 50,000 filas

**Acciones autom√°ticas:**
- Categorizar strings con <50% unique values
- Downcast int/float a tipos menores
- Resultado: 50-70% reducci√≥n memoria

**Referencia:** `/utils/memory-optimization.py`

---

## Checklist de An√°lisis

### ‚úÖ Pre-An√°lisis
- [ ] Periodo definido (fechas v√°lidas)
- [ ] Sites seleccionados
- [ ] Commerce Groups seleccionados
- [ ] Dimensi√≥n de an√°lisis definida
- [ ] Threshold configurado

### ‚úÖ Extracci√≥n
- [ ] Query ejecutada sin errores
- [ ] Volumen de datos esperado
- [ ] Exclusiones aplicadas correctamente
- [ ] AGRUP_COMMERCE calculado
- [ ] Threshold aplicado

### ‚úÖ Configuraci√≥n de Drivers
- [ ] Drivers configurados para todos los Site √ó Periodo
- [ ] Drivers > 0 para todas las combinaciones
- [ ] Drivers coherentes (variaci√≥n < 50% MoM)
- [ ] Fuentes de drivers documentadas

### ‚úÖ C√°lculo de CR
- [ ] CR calculado para todos los Site √ó Periodo
- [ ] CRs dentro de rango esperado (0.5-15.0 pp)
- [ ] No hay NULLs en CR
- [ ] Resultados validados

### ‚úÖ An√°lisis de Variaciones
- [ ] Variaci√≥n absoluta calculada
- [ ] Variaci√≥n relativa calculada
- [ ] Impacto en volumen calculado
- [ ] Patrones detectados

### ‚úÖ Interpretaci√≥n
- [ ] Top drivers identificados
- [ ] Distribuci√≥n temporal analizada
- [ ] Comparaci√≥n hist√≥rica realizada
- [ ] Insights generados
- [ ] Recomendaciones documentadas

---

## Referencias

- **Queries:** `/sql/base-query.sql`
- **C√°lculos:** `/calculations/contact-rate.py`
- **Patrones:** `/calculations/pattern-detection.py`
- **Drivers:** `/calculations/drivers-management.py`
- **Contexto:** `/docs/business-context.md`
- **Tablas:** `/docs/table-definitions.md`
- **M√©tricas:** `/docs/metrics-glossary.md`

---

**√öltima actualizaci√≥n:** Enero 2026  
**Versi√≥n:** 2.5 (Commerce)  
**Source:** V37.ipynb
